60-Year-Old Gave Himself Early 20th Century Psychosis After He Went To ChatGPT For Diet Advice
60-Year-Old Gave Himself Early 20th Century Psychosis After He Went To ChatGPT For Diet Advice The man, inspired by his nutrition studies in college, sought to eliminate a common ingredient found in food. By Pocharapon Neammanee Associate reporter Aug 8, 2025, 05:38 PM EDT LEAVE A COMMENT LOADINGERROR LOADING A 60-year-old man gave himself an uncommon psychiatric disorder after asking ChatGPT for diet advice in a case published Tuesday by the American College of Physicians Journals. The man, who remained anonymous in the case study, told doctors he had eliminated sodium chloride, commonly known as table salt, from his diet after reading about its negative health effects. He said he could only find sources telling him how to reduce salt, but not eliminate it completely. Advertisement Inspired by his nutrition studies in college, the man decided to completely eliminate sodium chloride from his diet as a personal experiment, with consultation from Chat GPT, researchers wrote. He maintained multiple dietary restrictions and even distilled his own water at home. “For 3 months, he had replaced sodium chloride with sodium bromide obtained from the internet after consultation with ChatGPT, in which he had read that chloride can be swapped with bromide, though likely for other purposes, such as cleaning,” the case study read. While excess sodium can raise blood pressure and increase the risk of health issues, it is still necessary to consume a healthy amount of it. Advertisement The man, who had no psychiatric history, eventually ended up at the hospital, worried that his neighbor was poisoning him. He told doctors he was very thirsty, but paranoid about the water he was offered. “In the first 24 hours of admission, he expressed increasing paranoia and auditory and visual hallucinations, which, after attempting to escape, resulted in an involuntary psychiatric hold for grave disability,” the study read. A man came down with a rare form of psychosis after ChatGPT gave him very bad dietary advice. Anadolu via Getty Images Advertisement Doctors concluded that the man was suffering from bromism, or bromide toxicity, a condition that is rare today but was more common in the early 20th century. The research noted that bromide was found in several over-the-counter medicines back then and contributed to up to 8% of bromism-related psychiatric admissions at that time. The hospital treated the man for psychosis and discharged him weeks later. His case highlights the potential pitfalls of using AI to seek medical tips. 20 Years OfFreeJournalism Your Support Fuels Our Mission Your Support Fuels Our Mission For two decades, HuffPost has been fearless, unflinching, and relentless in pursuit of the truth. Support our mission to keep us around for the next 20 — we can't do this without you. We remain committed to providing you with the unflinching, fact-based journalism everyone deserves. Thank you again for your support along the way. We’re truly grateful for readers like you! Your initial support helped get us here and bolstered our newsroom, which kept us strong during uncertain times. Now as we continue, we need your help more than ever. We hope you will join us once again. We remain committed to providing you with the unflinching, fact-based journalism everyone deserves. Thank you again for your support along the way. We’re truly grateful for readers like you! Your initial support helped get us here and bolstered our newsroom, which kept us strong during uncertain times. Now as we continue, we need your help more than ever. We hope you will join us once again. Support HuffPost Already contributed? Log in to hide these messages. Dr. Margaret Lozovatsky, a pediatrician, warned last year that AI often misses crucial context. “Even if the source is appropriate, when some of these tools are trying to combine everything into a summary, it’s often missing context clues, meaning it might forget a negative,” she told the American Medical Association. “So, it might forget the word ‘not’ and give you the opposite advice.” Advertisement Related Health Tech salt Artifical intelligence Chat GPT ChatGPT Was Asked To List Everyone Trump Has Called 'A Low-IQ Individual' — And It's Pretty Racist Elon Musk Soft Launches 'NSFW' AI Companion A Week After Chatbot Goes On Antisemitic Tirade These Tragic AI Fails Are Proof That You Can't Fully Rely On ChatGPT To Plan Your Trip Go to Homepage LEAVE A COMMENT Suggest a correction | Submit a tip Advertisement From Our Partner From Our Partner HuffPost Shopping's Best Finds Newsletter Sign Up The Morning Email Wake up to the day's most important news. Sign up for HuffPost's Morning Email. Successfully Signed Up! Realness delivered to your inbox By entering your email and clicking Sign Up, you're agreeing to let us send you customized marketing messages about us and our advertising partners. You are also agreeing to our Terms of Service and Privacy Policy. Close What's Hot More In U.S. News NEWSPOLITICSENTERTAINMENTLIFEVOICESHUFFPOST PERSONALSHOPPINGNEWSLETTERS About UsAdvertiseContact UsRSSFAQCareersUser AgreementComment PolicyDMCA PolicyAccessibility StatementPrivacy PolicyConsent PreferencesPrivacy Settings Part of HuffPost News. ©2025 BuzzFeed, Inc. All rights reserved. The Huffington Post